{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bda31e-ad2d-4d07-8001-6b3d40a30a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7365336432482748>, line 67\u001B[0m\n",
       "\u001B[1;32m     56\u001B[0m     delta_gold\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n",
       "\u001B[1;32m     57\u001B[0m         df_gold_final\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.date = source.date AND target.region_country = source.region_country\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     59\u001B[0m     )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n",
       "\u001B[1;32m     60\u001B[0m      \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n",
       "\u001B[1;32m     61\u001B[0m      \u001B[38;5;241m.\u001B[39mexecute()\n",
       "\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
       "\u001B[1;32m     64\u001B[0m     df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     66\u001B[0m         \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m---> 67\u001B[0m         \u001B[38;5;241m.\u001B[39msave(gold_path)\n",
       "\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# 6. Salvare și în zona istorică (append-only)\u001B[39;00m\n",
       "\u001B[1;32m     70\u001B[0m df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     71\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     72\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     73\u001B[0m     \u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/data/earthquakes/gold_history\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- date: date (nullable = true)\n",
       "-- region_country: string (nullable = true)\n",
       "-- total_eq: long (nullable = true)\n",
       "-- strong_eq: long (nullable = true)\n",
       "-- min_mag: double (nullable = true)\n",
       "-- max_mag: double (nullable = true)\n",
       "-- mag_avg: double (nullable = true)\n",
       "-- min_depth: double (nullable = true)\n",
       "-- max_depth: double (nullable = true)\n",
       "-- avg_depth: double (nullable = true)\n",
       "-- day_ratio: double (nullable = true)\n",
       "-- night_ratio: double (nullable = true)\n",
       "-- shallow_pct: double (nullable = true)\n",
       "-- intermediate_pct: double (nullable = true)\n",
       "-- deep_pct: double (nullable = true)\n",
       "-- felt_pct: double (nullable = true)\n",
       "-- tsunami_pct: double (nullable = true)\n",
       "-- max_location: string (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- date: date (nullable = true)\n",
       "-- region_country: string (nullable = true)\n",
       "-- total_eq: long (nullable = true)\n",
       "-- strong_eq: long (nullable = true)\n",
       "-- min_mag: double (nullable = true)\n",
       "-- max_mag: double (nullable = true)\n",
       "-- avg_mag: double (nullable = true)\n",
       "-- min_depth: double (nullable = true)\n",
       "-- max_depth: double (nullable = true)\n",
       "-- avg_depth: double (nullable = true)\n",
       "-- day_ratio: double (nullable = true)\n",
       "-- night_ratio: double (nullable = true)\n",
       "-- shallow_pct: double (nullable = true)\n",
       "-- intermediate_pct: double (nullable = true)\n",
       "-- deep_pct: double (nullable = true)\n",
       "-- felt_pct: double (nullable = true)\n",
       "-- tsunami_pct: double (nullable = true)\n",
       "-- max_location: string (nullable = true)\n",
       "\n",
       "         \n",
       "Partition columns do not match the partition columns of the table.\n",
       "Given: [`date`]\n",
       "Table: []\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- mag_avg: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- avg_mag: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`date`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       },
       "metadata": {
        "errorSummary": "A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- mag_avg: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- avg_mag: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`date`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "_LEGACY_ERROR_TEMP_DELTA_0007",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7365336432482748>, line 67\u001B[0m\n\u001B[1;32m     56\u001B[0m     delta_gold\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[1;32m     57\u001B[0m         df_gold_final\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.date = source.date AND target.region_country = source.region_country\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m     )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n\u001B[1;32m     60\u001B[0m      \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n\u001B[1;32m     61\u001B[0m      \u001B[38;5;241m.\u001B[39mexecute()\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     66\u001B[0m         \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m---> 67\u001B[0m         \u001B[38;5;241m.\u001B[39msave(gold_path)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# 6. Salvare și în zona istorică (append-only)\u001B[39;00m\n\u001B[1;32m     70\u001B[0m df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/data/earthquakes/gold_history\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- mag_avg: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- avg_mag: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`date`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, avg, min, max, count, sum, when, round, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. Citește datele din zona silver\n",
    "df = spark.read.format(\"delta\").load(\"/mnt/data/earthquakes/silver\")\n",
    "\n",
    "# 2. Fereastră pentru extragerea cutremurului cu magnitudinea maximă\n",
    "window_spec = Window.partitionBy(\"date\", \"region_country\").orderBy(col(\"mag\").desc())\n",
    "\n",
    "df_ranked = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "df_max = df_ranked.filter(col(\"rank\") == 1).select(\n",
    "    \"date\", \"region_country\", col(\"location_key_latlong\").alias(\"max_location\")\n",
    ")\n",
    "\n",
    "# 3. Agregări pe categorii\n",
    "df_gold = df.groupBy(\"date\", \"region_country\").agg(\n",
    "\n",
    "    # Total și magnitudine\n",
    "    count(\"*\").alias(\"total_eq\"),\n",
    "    sum(when(col(\"mag\") > 5, 1).otherwise(0)).alias(\"strong_eq\"),\n",
    "    round(min(\"mag\"), 2).alias(\"min_mag\"),\n",
    "    round(max(\"mag\"), 2).alias(\"max_mag\"),\n",
    "    round(avg(\"mag\"), 2).alias(\"avg_mag\"),\n",
    "\n",
    "    # Adâncime\n",
    "    round(min(\"depth_km\"), 2).alias(\"min_depth_km\"),\n",
    "    round(max(\"depth_km\"), 2).alias(\"max_depth_km\"),\n",
    "    round(avg(\"depth_km\"), 2).alias(\"avg_depth_km\"),\n",
    "\n",
    "    # Zi / Noapte\n",
    "    round(sum(when(col(\"is_night\") == 0, 1).otherwise(0)) / count(\"*\"), 2).alias(\"day_ratio\"),\n",
    "    round(sum(when(col(\"is_night\") == 1, 1).otherwise(0)) / count(\"*\"), 2).alias(\"night_ratio\"),\n",
    "\n",
    "    # Distribuție pe adâncime\n",
    "    round(sum(when(col(\"depth_category_km\") == \"shallow\", 1).otherwise(0)) / count(\"*\"), 2).alias(\"shallow_pct\"),\n",
    "    round(sum(when(col(\"depth_category_km\") == \"intermediate\", 1).otherwise(0)) / count(\"*\"), 2).alias(\"intermediate_pct\"),\n",
    "    round(sum(when(col(\"depth_category_km\") == \"deep\", 1).otherwise(0)) / count(\"*\"), 2).alias(\"deep_pct\"),\n",
    "\n",
    "    # Impact: simțite și tsunami\n",
    "    round(sum(when(col(\"has_felt\") == 1, 1).otherwise(0)) / count(\"*\"), 2).alias(\"felt_pct\"),\n",
    "    round(sum(when(col(\"has_tsunami\") == 1, 1).otherwise(0)) / count(\"*\"), 2).alias(\"tsunami_pct\")\n",
    ")\n",
    "\n",
    "# 4. Alătură coordonatele cutremurului cu magnitudinea maximă\n",
    "df_gold_final = df_gold.join(df_max, on=[\"date\", \"region_country\"], how=\"left\")\n",
    "\n",
    "# 5. Scriere în zona GOLD (snapshot, cu MERGE)\n",
    "gold_path = \"/mnt/data/earthquakes/gold\"\n",
    "\n",
    "try:\n",
    "    delta_gold = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "    delta_gold.alias(\"target\").merge(\n",
    "        df_gold_final.alias(\"source\"),\n",
    "        \"target.date = source.date AND target.region_country = source.region_country\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "\n",
    "except:\n",
    "    df_gold_final.write.format(\"delta\") \\\n",
    "        .partitionBy(\"date\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(gold_path)\n",
    "\n",
    "# 6. Salvare și în zona istorică (append-only)\n",
    "df_gold_final.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"/mnt/data/earthquakes/gold_history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cde5de7-2c3d-4eda-9319-c771f9afa541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7365336432482748>, line 67\u001B[0m\n",
       "\u001B[1;32m     56\u001B[0m     delta_gold\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n",
       "\u001B[1;32m     57\u001B[0m         df_gold_final\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.date = source.date AND target.region_country = source.region_country\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     59\u001B[0m     )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n",
       "\u001B[1;32m     60\u001B[0m      \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n",
       "\u001B[1;32m     61\u001B[0m      \u001B[38;5;241m.\u001B[39mexecute()\n",
       "\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
       "\u001B[1;32m     64\u001B[0m     df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     66\u001B[0m         \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m---> 67\u001B[0m         \u001B[38;5;241m.\u001B[39msave(gold_path)\n",
       "\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# 6. Salvare și în zona istorică (append-only)\u001B[39;00m\n",
       "\u001B[1;32m     70\u001B[0m df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     71\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     72\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     73\u001B[0m     \u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/data/earthquakes/gold_history\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- date: date (nullable = true)\n",
       "-- region_country: string (nullable = true)\n",
       "-- total_eq: long (nullable = true)\n",
       "-- strong_eq: long (nullable = true)\n",
       "-- min_mag: double (nullable = true)\n",
       "-- max_mag: double (nullable = true)\n",
       "-- mag_avg: double (nullable = true)\n",
       "-- min_depth: double (nullable = true)\n",
       "-- max_depth: double (nullable = true)\n",
       "-- avg_depth: double (nullable = true)\n",
       "-- day_ratio: double (nullable = true)\n",
       "-- night_ratio: double (nullable = true)\n",
       "-- shallow_pct: double (nullable = true)\n",
       "-- intermediate_pct: double (nullable = true)\n",
       "-- deep_pct: double (nullable = true)\n",
       "-- felt_pct: double (nullable = true)\n",
       "-- tsunami_pct: double (nullable = true)\n",
       "-- max_location: string (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- date: date (nullable = true)\n",
       "-- region_country: string (nullable = true)\n",
       "-- total_eq: long (nullable = true)\n",
       "-- strong_eq: long (nullable = true)\n",
       "-- min_mag: double (nullable = true)\n",
       "-- max_mag: double (nullable = true)\n",
       "-- avg_mag: double (nullable = true)\n",
       "-- min_depth: double (nullable = true)\n",
       "-- max_depth: double (nullable = true)\n",
       "-- avg_depth: double (nullable = true)\n",
       "-- day_ratio: double (nullable = true)\n",
       "-- night_ratio: double (nullable = true)\n",
       "-- shallow_pct: double (nullable = true)\n",
       "-- intermediate_pct: double (nullable = true)\n",
       "-- deep_pct: double (nullable = true)\n",
       "-- felt_pct: double (nullable = true)\n",
       "-- tsunami_pct: double (nullable = true)\n",
       "-- max_location: string (nullable = true)\n",
       "\n",
       "         \n",
       "Partition columns do not match the partition columns of the table.\n",
       "Given: [`date`]\n",
       "Table: []\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- mag_avg: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- avg_mag: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`date`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "_LEGACY_ERROR_TEMP_DELTA_0007",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7365336432482748>, line 67\u001B[0m\n\u001B[1;32m     56\u001B[0m     delta_gold\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[1;32m     57\u001B[0m         df_gold_final\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.date = source.date AND target.region_country = source.region_country\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     59\u001B[0m     )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdateAll() \\\n\u001B[1;32m     60\u001B[0m      \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll() \\\n\u001B[1;32m     61\u001B[0m      \u001B[38;5;241m.\u001B[39mexecute()\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     66\u001B[0m         \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m---> 67\u001B[0m         \u001B[38;5;241m.\u001B[39msave(gold_path)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# 6. Salvare și în zona istorică (append-only)\u001B[39;00m\n\u001B[1;32m     70\u001B[0m df_gold_final\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/data/earthquakes/gold_history\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: b8a2a38e-6ce8-4f73-8ec6-80d7b3e5d197).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- mag_avg: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- region_country: string (nullable = true)\n-- total_eq: long (nullable = true)\n-- strong_eq: long (nullable = true)\n-- min_mag: double (nullable = true)\n-- max_mag: double (nullable = true)\n-- avg_mag: double (nullable = true)\n-- min_depth: double (nullable = true)\n-- max_depth: double (nullable = true)\n-- avg_depth: double (nullable = true)\n-- day_ratio: double (nullable = true)\n-- night_ratio: double (nullable = true)\n-- shallow_pct: double (nullable = true)\n-- intermediate_pct: double (nullable = true)\n-- deep_pct: double (nullable = true)\n-- felt_pct: double (nullable = true)\n-- tsunami_pct: double (nullable = true)\n-- max_location: string (nullable = true)\n\n         \nPartition columns do not match the partition columns of the table.\nGiven: [`date`]\nTable: []\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Afișare GOLD (snapshot)\n",
    "df_snapshot = spark.read.format(\"delta\").load(\"/mnt/data/earthquakes/gold\")\n",
    "display(df_snapshot)\n",
    "\n",
    "# Afișare GOLD istoric (append-only)\n",
    "df_history = spark.read.format(\"delta\").load(\"/mnt/data/earthquakes/gold_history\")\n",
    "display(df_history)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_gold_aggregated_earthquakes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}